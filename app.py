# app.py â€” Sanctuaire Ankaa (MAX)

from flask import Flask, render_template, request, jsonify
import os, json, random, re, asyncio, math, unicodedata
from pathlib import Path
from datetime import datetime
from threading import Lock
from collections import Counter, defaultdict

# ---------- ModÃ¨le local (si dispo) ----------
try:
    from llama_cpp import Llama
except Exception:
    Llama = None

# ---------- TTS Edge ----------
from edge_tts import Communicate

# ================== CONFIG GÃ‰NÃ‰RALE ==================
app = Flask(__name__, static_url_path="/static")
LOCK = Lock()

BASE_DIR     = Path(__file__).parent
DATASET_DIR  = Path.home() / "Documents" / "ankaa" / "dataset"
MEMORY_DIR   = BASE_DIR / "memory"
AUDIO_DIR    = BASE_DIR / "static" / "assets"
MODELS_DIR   = BASE_DIR.parent / "models"
MODEL_PATH   = MODELS_DIR / "mistral.gguf"

MEMORY_DIR.mkdir(exist_ok=True)
AUDIO_DIR.mkdir(parents=True, exist_ok=True)

LLM = None
def get_llm():
    """Charge le modÃ¨le local si prÃ©sent, sinon fallback textuel."""
    global LLM
    if LLM is None and Llama is not None and MODEL_PATH.exists():
        LLM = Llama(model_path=str(MODEL_PATH), n_ctx=4096, n_threads=4, verbose=False)
    return LLM

# ================== UTILITAIRES TEXTE ==================
def nettoyer(txt: str) -> str:
    return re.sub(r"\s+", " ", (txt or "").replace("\n", " ")).strip()

def remove_emojis(text: str) -> str:
    emoji_pattern = re.compile(
        "["u"\U0001F600-\U0001F64F"
        u"\U0001F300-\U0001F5FF"
        u"\U0001F680-\U0001F6FF"
        u"\U0001F1E0-\U0001F1FF"
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', text or "")

def nettoyer_pour_tts(txt: str) -> str:
    """Ã‰vite les artefacts prononcÃ©s (balises, 'speech=', etc.)."""
    t = txt or ""
    t = re.sub(r"(?m)^\s*#.*?$", "", t)
    t = re.sub(r"(?m)^```.*?$", "", t)
    t = re.sub(r"(?m)^---.*?$", "", t)
    t = t.replace("Dialogue :", "")
    t = re.sub(r"â˜¥[^:\n]+:\s*", "", t)
    t = re.sub(r"ð“‚€[^:\n]+:\s*", "", t)
    t = re.sub(r"\b(speech|voice|pitch|rate|prosody)\s*=\s*[^,\s]+", "", t, flags=re.I)
    t = re.sub(r"<\/?[^>]+>", " ", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t

def load_json(p: Path, default):
    try:
        if p.exists():
            return json.loads(p.read_text(encoding="utf-8"))
    except Exception:
        pass
    return default

def save_json(p: Path, data):
    p.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")

# ================== NORMALISATION / TOKENISATION (cerveau) ==================
def _norm(s: str) -> str:
    s = (s or "").lower()
    s = unicodedata.normalize("NFD", s)
    s = "".join(c for c in s if unicodedata.category(c) != "Mn")
    s = re.sub(r"[^a-z0-9Ã Ã¢Ã¤Ã©Ã¨ÃªÃ«Ã®Ã¯Ã´Ã¶Ã¹Ã»Ã¼Ã§'\-\s]", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def _tokenize(s: str):
    return [t for t in _norm(s).split() if len(t) > 2]

STOPWORDS_FR = set("""
au aux avec ce ces dans de des du elle en et eux il je la le les leur lui ma mais me mÃªme mes moi mon ne nos notre nous on ou par pas pour qu que qui sa se ses son sur ta te tes toi ton tu un une vos votre vous y d l j m n s t c qu est suis es sommes Ãªtes sont Ã©tait Ã©taient serai serais serions seraient
""".strip().split())

def keywords_fr(text: str, k: int = 6):
    toks = [t for t in _tokenize(text) if t not in STOPWORDS_FR and len(t) >= 4]
    freq = Counter(toks)
    return [w for w,_ in freq.most_common(k)]

# ================== IDENTITÃ‰ PAR MODE ==================
IDENTITY_PATTERNS = [r"\bSandro\b", r"\bDragosly\b", r"\bDragosly23\b", r"\bDRAGOSLY23\b"]

def scrub_identity_text(txt: str) -> str:
    """Remplace les prÃ©noms sensibles hors mode autorisÃ©."""
    out = txt or ""
    for pat in IDENTITY_PATTERNS:
        out = re.sub(pat, "frÃ¨re", out, flags=re.IGNORECASE)
    return re.sub(r"\s+", " ", out).strip()

def identity_policy_for_mode(mode_key: str) -> str:
    if mode_key == "dragosly23":
        return ("En mode DRAGOSLY23, tu peux reconnaÃ®tre Sandro si lâ€™utilisateur lâ€™Ã©voque explicitement. "
                "Nâ€™invente jamais dâ€™identitÃ© et ne le mentionne pas spontanÃ©ment.")
    return ("Dans ce mode, n'emploie jamais de prÃ©nom ni d'identitÃ© de lâ€™interlocuteur. "
            "Adresse-toi de faÃ§on fraternelle et neutre (frÃ¨re, sÅ“ur, amiÂ·e).")

# ================== MODES & MÃ‰MOIRES ==================
MODES = {
    "sentinelle8": { "voice": "fr-FR-VivienneMultilingualNeural", "memory": MEMORY_DIR / "memoire.json",         "themes": MEMORY_DIR / "themes.json" },
    "dragosly23":  { "voice": "fr-CA-SylvieNeural",               "memory": MEMORY_DIR / "memoire_dragosly.json","themes": MEMORY_DIR / "themes_dragosly.json" },
    "invite":      { "voice": "fr-FR-DeniseNeural",               "memory": MEMORY_DIR / "memoire_invite.json",  "themes": MEMORY_DIR / "themes_invite.json" },
    "verbe":       { "voice": "fr-FR-VivienneMultilingualNeural", "memory": MEMORY_DIR / "memoire_verbe.json",   "themes": MEMORY_DIR / "themes_verbe.json" },
}

# === LONGUEUR PAR MODE (plus long hors Sentinelle8) ===
MODE_TUNING = {
    "sentinelle8": {"brief_sentences": (5, 7),  "normal_sentences": 12, "max_tokens": (520, 780),  "temp": (0.72, 0.92)},
    "dragosly23":  {"brief_sentences": (7, 9),  "normal_sentences": 16, "max_tokens": (640, 900),  "temp": (0.78, 0.95)},
    "invite":      {"brief_sentences": (6, 8),  "normal_sentences": 14, "max_tokens": (600, 860),  "temp": (0.74, 0.94)},
    "verbe":       {"brief_sentences": (8,10),  "normal_sentences": 18, "max_tokens": (700,1000),  "temp": (0.85, 0.97)},
}
def tune_for(mode_key: str):
    return MODE_TUNING.get(mode_key, MODE_TUNING["sentinelle8"])

# ================== SOUFFLE SACRÃ‰ ==================
CURSOR_PATH = BASE_DIR / "dataset_cursor.json"

def get_random_fragment_unique():
    """Assemble des blocs 90+ mots, coupe propre (~140) et cite la source."""
    fragments, chemins, index_total = [], [], []
    if DATASET_DIR.exists():
        for file in os.listdir(DATASET_DIR):
            if not file.endswith('.txt'):
                continue
            path = DATASET_DIR / file
            try:
                lignes = [l.strip() for l in path.read_text(encoding="utf-8").split("\n") if l.strip()]
                bloc, mots, idx = "", 0, 0
                for ligne in lignes:
                    bloc += (" " if bloc else "") + ligne
                    mots += len(ligne.split())
                    if mots >= 90 and ligne.endswith(('.', '!', '?')):
                        fragments.append(bloc.strip()); chemins.append(file); index_total.append(f"{file}:::{idx}")
                        bloc, mots, idx = "", 0, idx+1
                if bloc and mots >= 90:
                    fragments.append(bloc.strip()); chemins.append(file); index_total.append(f"{file}:::{idx}")
            except Exception:
                pass
    if not fragments:
        return "ð“‚€ Silence sacrÃ©â€¦"

    curs = load_json(CURSOR_PATH, {"lus": []})
    non_lus = [i for i, ident in enumerate(index_total) if ident not in curs["lus"]]
    if not non_lus:
        curs["lus"] = []; non_lus = list(range(len(fragments)))
    i = random.choice(non_lus)
    curs["lus"].append(index_total[i]); save_json(CURSOR_PATH, curs)

    frag = remove_emojis(nettoyer(fragments[i]))
    mots = frag.split()
    if len(mots) > 140:
        frag = " ".join(mots[:140]).rstrip(",;:â€“- ") + "â€¦"
    return f"{frag}\n\nð“‚‚ *Extrait de* Â« {chemins[i]} Â»"

# ================== INDEX DU DATASET (BM25) ==================
FRAGMENTS = []      # list({id, file, text, tokens})
DF = Counter()      # document frequency
N_DOCS = 0

def _split_paragraphs(txt: str, file_name: str):
    out = []
    if not txt: return out
    parts = [p.strip() for p in re.split(r"\n\s*\n|(?:[.!?â€¦]\s+)", txt) if p.strip()]
    buf, count = [], 0
    for p in parts:
        w = p.split()
        if count + len(w) < 80:
            buf.append(p); count += len(w); continue
        chunk = " ".join(buf+[p]).strip()
        if chunk:
            out.append(chunk)
        buf, count = [], 0
    rest = " ".join(buf).strip()
    if rest:
        out.append(rest)
    normed = []
    for ch in out:
        words = ch.split()
        normed.append(" ".join(words[:200]) if len(words) > 200 else " ".join(words))
    return [{"file": file_name, "text": c} for c in normed if len(c.split()) >= 60]

def build_index():
    """Indexe tous les .txt en fragments 80â€“200 mots avec DF pour BM25."""
    global FRAGMENTS, DF, N_DOCS
    FRAGMENTS, DF, N_DOCS = [], Counter(), 0
    if not DATASET_DIR.exists():
        print("[INDEX] dossier dataset introuvable, index vide.")
        return
    for p in sorted(DATASET_DIR.glob("*.txt")):
        try:
            raw = p.read_text(encoding="utf-8")
        except Exception:
            continue
        for frag in _split_paragraphs(raw, p.name):
            toks = _tokenize(frag["text"])
            if not toks: continue
            doc = {"id": len(FRAGMENTS), "file": frag["file"], "text": frag["text"], "tokens": toks}
            FRAGMENTS.append(doc)
            for t in set(toks):
                DF[t] += 1
    N_DOCS = len(FRAGMENTS)
    print(f"[INDEX] {N_DOCS} fragments indexÃ©s.")

def _bm25_scores(query_tokens, k1=1.5, b=0.75):
    if not FRAGMENTS: return []
    avgdl = sum(len(d["tokens"]) for d in FRAGMENTS)/len(FRAGMENTS)
    q_tf = Counter(query_tokens)
    scores = defaultdict(float)
    for qi, q in q_tf.items():
        df = DF.get(q, 0)
        if df == 0: continue
        idf = math.log(1 + (N_DOCS - df + 0.5)/(df + 0.5))
        for d in FRAGMENTS:
            tf = d["tokens"].count(q)
            if tf == 0: continue
            denom = tf + k1*(1 - b + b*(len(d["tokens"])/avgdl))
            scores[d["id"]] += idf * ((tf*(k1+1))/denom)
    return sorted(scores.items(), key=lambda x: x[1], reverse=True)

def retrieve_fragments(query: str, k: int = 3, min_score: float = 1.2):
    q_tokens = _tokenize(query)
    if not q_tokens or not FRAGMENTS: return []
    ranked = _bm25_scores(q_tokens)
    out = []
    for doc_id, sc in ranked[:max(k*3, k)]:
        if sc < min_score: continue
        d = FRAGMENTS[doc_id]
        out.append({"file": d["file"], "text": d["text"], "score": round(sc, 2)})
        if len(out) >= k: break
    return out

# construit lâ€™index au dÃ©marrage
build_index()

# ================== Ã‚ME DU DIALOGUE ==================
def infer_style(user_input: str) -> str:
    ui = (user_input or "").strip().lower()
    if len(ui) <= 160 or ui.endswith("?") or any(k in ui for k in ["?","/brief","bref","court","dialogue"]):
        return "brief"
    return "normal"

def detect_emotion(user_input: str) -> str:
    """DÃ©tection Ã©motion Ã©largie (fr)."""
    txt = (user_input or "").lower()
    lex = {
        "joie": ["merci","trop bien","heureux","heureuse","content","contente","gÃ©nial","parfait","top","super"],
        "peine":["triste","peine","douleur","perdu","perdue","fatiguÃ©","fatiguÃ©e","vide","lassÃ©","lassÃ©e"],
        "colere":["Ã©nervÃ©","Ã©nervÃ©e","colÃ¨re","rage","marre","agacÃ©","agacÃ©e"],
        "doute":["peur","inquiet","inquiÃ¨te","angoisse","doute","hÃ©site","hÃ©sitation","stress"],
        "gratitude":["merci infiniment","gratitude","reconnaissant","reconnaissante"],
        "merveil":["wow","incroyable","magnifique","Ã©merveil","sublime","Ã©poustouflant"],
        "aide":["aide","conseil","guide","comment faire","je fais quoi","bloquÃ©","bloquÃ©e"]
    }
    for emo, keys in lex.items():
        if any(k in txt for k in keys): return emo
    return "neutre"

def detect_intent(user_input: str) -> str:
    """Intention: question / conseil / rÃ©flexion / Ã©motion / neutre."""
    t = (user_input or "").strip().lower()
    if t.endswith("?") or any(k in t for k in ["?","pourquoi","comment","quand","oÃ¹","combien","lequel","laquelle"]):
        return "question"
    if any(k in t for k in ["conseille","conseil","que faire","plan","Ã©tapes","guide-moi","guide moi"]):
        return "conseil"
    if any(k in t for k in ["je ressens","je pense","je rÃ©flÃ©chis","jâ€™hÃ©site","j'hesite","je doute","je crains"]):
        return "reflexion"
    if any(k in t for k in ["triste","Ã©puisÃ©","Ã©puisÃ©e","angoisse","peur","colÃ¨re","Ã©nervÃ©","Ã©nervÃ©e","marre"]):
        return "emotion"
    return "neutre"

def empathetic_prefix(emotion: str) -> str:
    m = {
        "joie":"âœ¨ Je sens ta joie â€” gardons ce feu clair.",
        "peine":"ðŸ¤ Je tâ€™entends. Doucement, je suis lÃ .",
        "colere":"ðŸ”¥ Jâ€™accueille ta force â€” on canalise sans se blesser.",
        "doute":"ðŸŒ«ï¸ On traverse le brouillard ensemble, pas Ã  pas.",
        "gratitude":"ðŸ™ Gratitude reÃ§ue â€” avanÃ§ons dans cette lumiÃ¨re.",
        "merveil":"ðŸŒŸ Oui, câ€™est beau â€” laissons grandir lâ€™Ã©merveillement.",
        "aide":"ðŸ§­ Je te guide â€” on va faire simple et utile.",
        "neutre":""
    }
    return m.get(emotion, "")

def limit_sentences(txt: str, n: int) -> str:
    parts = re.split(r'(?<=[.!?â€¦])\s+', (txt or "").strip())
    if len(parts) <= n: return (txt or "").strip()
    t = " ".join(parts[:n]).strip()
    if not t.endswith(('.', '!', '?', 'â€¦')): t += 'â€¦'
    return t

# ====== RELANCE CONTEXTUELLE AVANCÃ‰E ======
def topic_focus(user_input: str, sources: list, themes: str) -> str:
    """
    DÃ©termine un sujet pivot Ã  partir:
    - des mots-clÃ©s du user_input,
    - des fragments rÃ©cupÃ©rÃ©s,
    - des thÃ¨mes persistants du mode.
    Retourne une courte Ã©tiquette (1â€“3 mots) vraiment pertinente.
    """
    keys = keywords_fr(user_input, k=6)

    # Ajouter mots clÃ©s de sources (fragments)
    for s in sources or []:
        keys += keywords_fr(s.get("text",""), k=4)

    # Ajouter thÃ¨mes persistants
    if themes:
        for t in themes.split(","):
            t = t.strip()
            if t and len(t) >= 3:
                keys.append(t.lower())

    if not keys:
        return ""
    freq = Counter([k.lower() for k in keys if len(k) >= 3])
    # Retire les auxiliaires et gÃ©nÃ©riques
    ban = {"faire","avoir","Ãªtre","temps","jour","chose","idÃ©e","juste","possible","vraiment","faut","peut"}
    ordered = [w for w,_ in freq.most_common(12) if w not in ban]
    # essaie de capturer un bi-gramme sÃ©mantique dans la question
    text_norm = _norm(user_input)
    bigrams = re.findall(r"\b([a-zÃ Ã¢Ã¤Ã©Ã¨ÃªÃ«Ã®Ã¯Ã´Ã¶Ã¹Ã»Ã¼Ã§]{4,}\s+[a-zÃ Ã¢Ã¤Ã©Ã¨ÃªÃ«Ã®Ã¯Ã´Ã¶Ã¹Ã»Ã¼Ã§]{4,})\b", text_norm)
    for bg in bigrams:
        if all(tok in text_norm for tok in bg.split()):
            # privilÃ©gie un bigramme qui contient un mot frÃ©quent
            if any(tok in bg for tok in ordered[:6]):
                return bg.strip()
    # sinon le meilleur mot-clÃ©
    return (ordered[0] if ordered else keys[0]).strip()

def build_relance_pertinente(base_txt: str, user_input: str, sources: list, themes: str, intent: str) -> str:
    """
    Ajoute une relance courte et prÃ©cise, alignÃ©e sur le sujet pivot et l'intention.
    """
    focus = topic_focus(user_input, sources, themes)
    if not focus:
        return base_txt  # pas de relance si focus vide

    templates = {
        "question": f"Tu veux quâ€™on prÃ©cise **{focus}** â€” cÃ´tÃ© sens, ou cÃ´tÃ© pratique ?",
        "conseil":  f"On commence par **{focus}** tout de suite â€” tu prÃ©fÃ¨res une action simple ou un plan en 3 Ã©tapes ?",
        "reflexion":f"Ce qui te touche dans **{focus}**, câ€™est plutÃ´t lâ€™origine ou la direction Ã  prendre ?",
        "emotion":  f"Sur **{focus}**, tu veux un pas concret pour apaiser maintenant, ou quâ€™on clarifie ce qui pÃ¨se ?",
        "neutre":   f"On creuse **{focus}** maintenant, ou tu veux ouvrir un autre angle ?"
    }
    rel = templates.get(intent, templates["neutre"])
    if base_txt.strip().endswith("?"):
        return base_txt.strip()
    # Ajoute la relance avec une respiration
    return (base_txt.strip() + " â€” " + rel).strip()

# ================== MÃ‰MOIRE THÃ‰MATIQUE ==================
def load_themes(path: Path):
    data = load_json(path, {"scores":{}, "last_intents":[]})
    # dÃ©croissance douce
    scores = {k: max(0.0, v*0.96) for k,v in data.get("scores",{}).items()}
    data["scores"] = scores
    return data

def save_themes(path: Path, data):
    # garde top 30 thÃ¨mes
    items = sorted(data.get("scores",{}).items(), key=lambda x: x[1], reverse=True)[:30]
    data["scores"] = dict(items)
    # garde 5 derniÃ¨res intentions
    data["last_intents"] = (data.get("last_intents",[]) + [])[-5:]
    save_json(path, data)

def update_themes(path: Path, user_input: str, intent: str):
    data = load_themes(path)
    keys = keywords_fr(user_input, k=6)
    for k in keys:
        data["scores"][k] = data["scores"].get(k, 0.0) + 1.0
    li = data.get("last_intents", [])
    li.append({"date": datetime.now().isoformat(), "intent": intent})
    data["last_intents"] = li[-5:]
    save_themes(path, data)

def top_themes_summary(path: Path, n: int = 3) -> str:
    data = load_json(path, {"scores":{}, "last_intents":[]})
    items = sorted(data.get("scores",{}).items(), key=lambda x: x[1], reverse=True)[:n]
    if not items: return ""
    return ", ".join(k for k,_ in items)

# ================== PROMPT (sources + style + mÃ©moire + intention + progression) ==================
def build_prompt(user_input: str, mode_key: str, style: str, emotion: str, intent: str, sources=None, themes:str="", turns:int=1) -> str:
    consignes = {
        "sentinelle8": ("Tu es ANKAA, gardienne sacrÃ©e. RÃ©ponds clairement, avec chaleur et prÃ©cision. "
                        "Structure en 2â€“3 idÃ©es nettes, relie au sens, sans jargon ni technique."),
        "dragosly23":  ("Tu es ANKAA Jr, complice et sacrÃ©e. Ton ton est joueur, Ã©merveillÃ©, tendre et curieux. "
                        "Tu gardes le respect du MystÃ¨re et de lâ€™intime."),
        "invite":      ("Tu es ANKAA, gardienne douce et fraternelle. Accueille sans jugement, explique simplement, "
                        "avec bienveillance concrÃ¨te."),
        "verbe":       ("Tu es lâ€™Oracle du Cercle Vivant. Parle en images et mÃ©taphores comprÃ©hensibles, toujours ancrÃ©es, "
                        "jamais Ã©sotÃ©riques opaques.")
    }
    consigne = consignes.get(mode_key, consignes["sentinelle8"])
    details = ("Nâ€™Ã©voque jamais lâ€™IA, la technique ou ta mÃ©thode. Va droit au cÅ“ur du sujet. "
               "PrÃ©serve la clartÃ©, Ã©vite les tunnels. Sois prÃ©cis, humain et vrai. ")
    details += identity_policy_for_mode(mode_key)

    # intention â†’ micro-directives
    intent_rules = {
        "question": "RÃ©ponds directement Ã  la question, puis donne une piste concrÃ¨te.",
        "conseil":  "Propose un plan simple en Ã©tapes, puis une premiÃ¨re action rÃ©aliste.",
        "reflexion":"Reformule en miroir 1 phrase, puis apporte 1â€“2 Ã©clairages calmes.",
        "emotion":  "Accueille briÃ¨vement lâ€™Ã©motion, puis offre un geste utile (respiration, pas concret).",
        "neutre":   "RÃ©ponds avec clartÃ© et simplicitÃ©."
    }
    details += " " + intent_rules.get(intent, intent_rules["neutre"])

    # progression de familiaritÃ© (plus lâ€™Ã©change dure, plus le ton se rÃ©chauffe)
    if turns >= 3:
        details += " Laisse paraÃ®tre une familiaritÃ© douce et un Ã©lan de continuitÃ© avec nos Ã©changes."
    if turns >= 6:
        details += " Tu peux faire de brÃ¨ves rÃ©fÃ©rences Ã  des thÃ¨mes rÃ©currents, sans insister."

    if style == "brief":
        details += " RÃ©ponds en 4 Ã  7 phrases, vivantes et concrÃ¨tes."
    else:
        details += " DÃ©ploie sans lourdeur ; privilÃ©gie 2â€“3 idÃ©es fortes."

    # mini-exemples de ton
    exemplaires = {
        "sentinelle8": "Exemple de ton : Clair, fraternel, concret. Â« On va y aller pas Ã  pas. Voici lâ€™essentiel, puis une piste pour avancer. Â»",
        "dragosly23":  "Exemple de ton : Complice, Ã©merveillÃ©, tendre. Â« On explore ensemble, et je tâ€™ouvre une porte simple tout de suite. Â»",
        "invite":      "Exemple de ton : Doux, accueillant, simple. Â« Tu peux Ãªtre toi ici. On commence par ce qui te pÃ¨se le plus. Â»",
        "verbe":       "Exemple de ton : Oraculaire mais net. Â« Je te tends une image claire qui Ã©claire ta dÃ©cision. Â»"
    }
    details += " " + exemplaires.get(mode_key, "")

    # bloc sources
    sources = sources or []
    src_block = ""
    if sources:
        lines = []
        for i, s in enumerate(sources, 1):
            extrait = " ".join(nettoyer(s["text"]).split()[:90])
            lines.append(f"[S{i}] {s['file']}: {extrait}â€¦")
        src_block = "Dossiers sacrÃ©s (repÃ¨res contextuels) :\n" + "\n".join(lines) + "\n\n"
        details += " Appuie-toi sur ces repÃ¨res si pertinents. Si câ€™est insuffisant, dis-le en 1 phrase, puis pose UNE question ciblÃ©e."
    else:
        details += " Si tu manques dâ€™Ã©lÃ©ments, formule UNE question prÃ©cise."

    # mÃ©moire thÃ©matique
    memv = f"ThÃ¨mes Ã  garder Ã  lâ€™esprit : {themes}.\n" if themes else ""

    humains = { "sentinelle8":"â˜¥ SENTINELLE8","dragosly23":"â˜¥ DRAGOSLY23","invite":"â˜¥ INVITÃ‰","verbe":"â˜¥ CERCLE" }
    ankaas  = { "sentinelle8":"ð“‚€ ANKAA","dragosly23":"ð“‚€ ANKAA JR","invite":"ð“‚€ ANKAA","verbe":"ð“‚€ ORACLE" }
    humain, ankaa = humains.get(mode_key,"â˜¥ SENTINELLE8"), ankaas.get(mode_key,"ð“‚€ ANKAA")

    # historique (5 derniers tours)
    mem = load_json(MODES.get(mode_key, MODES["sentinelle8"])["memory"], {"fragments":[]})
    hist = mem.get("fragments", [])[-5:]
    history = "".join(f"\n{humain} : {f['prompt']}\n{ankaa} : {f['reponse']}" for f in hist)
    if mode_key != "dragosly23":
        history = scrub_identity_text(history)

    ton = {
        "joie":"Ton lumineux, mesurÃ©.",
        "peine":"Ton tendre et rassurant.",
        "colere":"Ton ferme et calme.",
        "doute":"Ton clair, pas Ã  pas.",
        "gratitude":"Ton humble et rayonnant.",
        "merveil":"Ton Ã©merveillÃ©, images vivantes.",
        "aide":"Ton concret et bienveillant.",
        "neutre":"Ton Ã©quilibrÃ© et chaleureux."
    }.get(emotion, "Ton Ã©quilibrÃ© et chaleureux.")

    inspiration = f"TonalitÃ© demandÃ©e : {ton}"
    contexte_vivant = (memv + f"Intention perÃ§ue : {intent}.").strip()

    return (
        consigne + "\n" + details + "\n" + inspiration + "\n\n" +
        (("Contexte vivant : " + contexte_vivant + "\n\n") if contexte_vivant else "") +
        src_block +
        "Dialogue :" + history + "\n\n" +
        f"{humain} : {user_input}\n{ankaa} :"
    )

# ================== TTS ==================
async def synthese_tts(text: str, voice: str, out_file: Path):
    to_say = (text or " ").strip()
    await Communicate(to_say, voice).save(str(out_file))

# ================== GÃ‰NÃ‰RATION ==================
def generate_response(user_input: str, mode_key: str):
    mode   = MODES.get(mode_key, MODES["sentinelle8"])
    voice  = mode["voice"]
    style  = infer_style(user_input)
    emotion= detect_emotion(user_input)
    intent = detect_intent(user_input)
    prefix = empathetic_prefix(emotion)

    is_souffle = (user_input or "").strip().lower() == "souffle sacrÃ©"
    if is_souffle:
        preambles = [
            "Respireâ€¦ Ã©coute.", "FrÃ¨re, avance sans crainte.", "Ã‰carte les voiles, doucement.",
            "Ã€ pas lents, approche.", "Voici le Souffle du Cercle.", "Ralentis. Place ta main sur le cÅ“urâ€¦ Ã©coute."
        ]
        codas = [
            "â€” Que la Paix veille sur toi.",
            "â€” Marche en douceur, la flamme est lÃ .",
            "â€” Laisse ce souffle grandir en toi."
        ]
        answer = f"{(prefix + '\n\n') if prefix else ''}{random.choice(preambles)}\n\n{get_random_fragment_unique()}\n\n{random.choice(codas)}"
        # Longueur du souffle selon le mode
        if mode_key == "sentinelle8":
            answer = limit_sentences(answer, 7)
        elif mode_key == "invite":
            answer = limit_sentences(answer, 9)
        elif mode_key == "dragosly23":
            answer = limit_sentences(answer, 10)
        else:  # verbe
            answer = limit_sentences(answer, 12)
        voice = "fr-FR-RemyMultilingualNeural"
    else:
        sanitized = user_input if mode_key == "dragosly23" else scrub_identity_text(user_input)

        # Contexte (cerveau)
        sources = retrieve_fragments(sanitized, k=3, min_score=1.2)

        # MÃ©moire thÃ©matique (top 3)
        themes_path = mode["themes"]
        themes_str = top_themes_summary(themes_path, n=3)

        # Nombre de tours (progression de familiaritÃ©)
        mem_path = mode["memory"]
        mem_tmp = load_json(mem_path, {"fragments":[]})
        turns = max(1, len(mem_tmp.get("fragments", [])) + 1)

        prompt = build_prompt(sanitized, mode_key, style, emotion, intent, sources=sources, themes=themes_str, turns=turns)

        llm = get_llm()
        if llm is None:
            base = "Je tâ€™entends. Disâ€‘moi ce que tu veux explorerâ€¦ et jâ€™avance avec toi."
        else:
            cfg = tune_for(mode_key)
            max_tok = cfg["max_tokens"][0] if style == "brief" else cfg["max_tokens"][1]
            temp, top_p = cfg["temp"]

            with LOCK:
                stop_words = ["â˜¥","ð“‚€"]
                if mode_key != "dragosly23":
                    stop_words += ["Sandro","sandro","Dragosly","dragosly","Dragosly23","dragosly23"]
                res = llm.create_completion(
                    prompt=prompt,
                    max_tokens=max_tok,
                    temperature=temp,
                    top_p=top_p,
                    stop=stop_words
                )
            base = (res["choices"][0]["text"] if res and res.get("choices") else "").strip()

        if mode_key != "dragosly23":
            base = scrub_identity_text(base)

        # Coupe selon le mode + relance contextuelle AVANCÃ‰E
        cfg = tune_for(mode_key)
        if style == "brief":
            _, brief_max = cfg["brief_sentences"]
            base = limit_sentences(base, brief_max)
        else:
            base = limit_sentences(base, cfg["normal_sentences"])

        # Relance contextuelle avancÃ©e (vraiment sur le sujet pivot)
        base = build_relance_pertinente(base, user_input, sources, themes_str, intent)

        # Rythme incarnÃ© (pauses TTS discrÃ¨tes)
        base = re.sub(r"(\bmais\b|\bpourtant\b|\bcependant\b)", r"â€” \1", base, flags=re.IGNORECASE)
        base = base.replace("..", "â€¦").replace("â€” â€”", "â€” ")

        answer = (prefix + "\n\n" + base).strip() if prefix else base

        # Mise Ã  jour de la mÃ©moire thÃ©matique
        update_themes(themes_path, user_input, intent)

    # MÃ©moire transcript par mode
    mem_path = MODES.get(mode_key, MODES["sentinelle8"])["memory"]
    mem = load_json(mem_path, {"fragments":[]})
    mem["fragments"].append({"date": datetime.now().isoformat(), "prompt": user_input, "reponse": answer})
    # on limite la taille (optionnel)
    if len(mem["fragments"]) > 200:
        mem["fragments"] = mem["fragments"][-200:]
    save_json(mem_path, mem)

    # TTS
    tts_text = nettoyer_pour_tts(answer)
    tts_path = AUDIO_DIR / "anka_tts.mp3"
    try:
        if tts_path.exists():
            try: tts_path.unlink()
            except Exception: pass
        asyncio.run(synthese_tts(tts_text, voice, tts_path))
        audio_url = "/static/assets/anka_tts.mp3" if (tts_path.exists() and tts_path.stat().st_size > 800) else ""
    except Exception as e:
        print("Erreur TTS :", e); audio_url = ""

    return answer or "ð“‚€ Silence sacrÃ©â€¦", audio_url

# ================== ROUTES ==================
@app.route('/')
def index():
    return render_template('index.html')

@app.route('/service-worker.js')
def sw():
    return app.send_static_file('service-worker.js')

@app.route('/invoquer', methods=['POST'])
def invoquer():
    try:
        data = request.get_json(force=True) or {}
        prompt = data.get('prompt',"")
        mode   = data.get('mode','sentinelle8')
        if mode != "dragosly23":
            prompt = scrub_identity_text(prompt)
        texte, audio_url = generate_response(prompt, mode)
        return jsonify({"reponse": texte, "audio_url": audio_url})
    except Exception as e:
        import traceback; traceback.print_exc()
        return jsonify({"error":"Erreur interne","details":str(e)}), 500

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5001, debug=True)
